# Smart Librarian — RAG + Tools + Web UI (FastAPI)

A book-recommendation chatbot that combines **Retrieval-Augmented Generation (RAG)** over a local dataset with a **tool** for pulling full summaries, and a lightweight **web UI** (FastAPI + HTML/CSS/JS) offering optional **speech-to-text (STT)** and **text-to-speech (TTS)**.

- **Vector store:** ChromaDB (local, non-OpenAI)  
- **Embeddings:** OpenAI (configurable)  
- **Tool calling:** `get_summary_by_title` (returns full summary from local JSON)  
- **Web UI:** Chat-style interface with browser STT/TTS via Web Speech API  
- **Optional server-side:** TTS (`gpt-4o-mini-tts`) and STT (Whisper)  

---

## Table of Contents
- [Features](#features)
- [Architecture](#architecture)
- [Requirements](#requirements)
- [Installation](#installation)
- [Configuration](#configuration)
- [Dataset](#dataset)
- [Build the Vector Store (Ingestion)](#build-the-vector-store-ingestion)
- [Quick Tests (CLI)](#quick-tests-cli)
- [Run the Web Server](#run-the-web-server)
- [REST API](#rest-api)
- [Project Structure](#project-structure)
- [Tips & Troubleshooting](#tips--troubleshooting)
- [Roadmap](#roadmap)
- [License](#license)

---

## Features

- **RAG on local ChromaDB** for low-latency, local retrieval (HNSW, cosine).  
- **Deterministic tool call**: `get_summary_by_title` pulls the full book summary from your dataset for high-quality answers.  
- **Simple chat UI** (FastAPI served page) with **in-browser STT/TTS**; optional **server-side** TTS/STT endpoints for consistent quality.  
- **Content moderation**: basic language filter before hitting the LLM.  

---

## Architecture

User → Web UI (FastAPI + HTML/JS)
│
▼
/api/chat
│
▼
Retrieval over ChromaDB ←— book_summaries.json (local)
│ top-K
▼
LLM selects candidate title
│
├── tool: get_summary_by_title → returns full summary
▼
Compose final answer + sources

Optional:

/api/tts (server-side TTS with gpt-4o-mini-tts)

/api/stt (server-side STT with Whisper)

yaml
Copy code

---

## Requirements

- **Python** 3.10+  
- **OpenAI API key** stored in `.env` at the project root.  
- Optional for `/api/stt` uploads: `python-multipart`.  

---

## Installation

```bash
# Clone
git clone https://github.com/Iulius2002/smart-librarian.git
cd smart-librarian

# Install Python deps
pip install -r requirements.txt

# If you plan to POST audio to /api/stt:
pip install python-multipart
Configuration
Create a .env file at the project root:

dotenv
Copy code
OPENAI_API_KEY=sk-...your-key...
CHROMA_DIR=data/chroma_db
.env is already ignored by git; data/chroma_db is a local persistence folder for Chroma.

Dataset
Place your dataset (≥ 10 books) in data/:

book_summaries.json — full summaries (used by RAG + tool)

book_summaries.md — short summaries (for display/theme)

Re-run ingestion whenever you add/update books.

Build the Vector Store (Ingestion)
bash
Copy code
python -m src.ingest
This will populate data/chroma_db/ (not committed).

Quick Tests (CLI)
Test retrieval (no LLM):

bash
Copy code
python -m src.test_retrieval
Example queries:

„o carte despre libertate și control social”

„prietenie și magie”

„război și destine”

End-to-end chat (RAG + LLM + tool):

bash
Copy code
python -m src.test_chat
Run the Web Server
bash
Copy code
uvicorn src.server:app --reload
Open the UI at: http://127.0.0.1:8000/

In the UI you can:

type or dictate questions,

toggle automatic TTS playback,

pick a ro-RO voice in your system for natural pronunciation (if available).

REST API
POST /api/chat
Performs RAG + LLM + tool and returns the final answer + top-3 source snippets.

Request

json
Copy code
{ "message": "Vreau o carte despre prietenie și magie" }
Response

json
Copy code
{
  "answer": "<final text: recomandare + rezumat complet>",
  "sources": [
    { "title": "The Hobbit", "preview": "..." },
    { "title": "The Lord of the Rings", "preview": "..." },
    { "title": "Pride and Prejudice", "preview": "..." }
  ]
}
POST /api/tts (optional — server-side TTS)
Generates MP3 from text using gpt-4o-mini-tts.

curl example

bash
Copy code
curl -s -X POST http://127.0.0.1:8000/api/tts \
  -H "Content-Type: application/json" \
  -d '{"text":"Acesta este un test în limba română."}' \
  --output tts.mp3
POST /api/stt (optional — server-side STT)
Transcribes audio using Whisper (whisper-1).

curl example

bash
Copy code
curl -s -X POST http://127.0.0.1:8000/api/stt \
  -F "audio=@/path/to/recording.m4a"
Project Structure
pgsql
Copy code
smart-librarian/
├─ README.md
├─ requirements.txt
├─ .env                # (gitignored)
├─ data/
│  ├─ book_summaries.json
│  ├─ book_summaries.md
│  └─ chroma_db/       # generated by Chroma (gitignored)
└─ src/
   ├─ __init__.py
   ├─ config.py
   ├─ vector_store.py
   ├─ ingest.py
   ├─ tools.py
   ├─ chat.py
   ├─ test_retrieval.py
   ├─ test_chat.py
   ├─ moderation.py
   ├─ server.py
   └─ templates/
      └─ index.html
Tips & Troubleshooting
.env not found → run commands from the project root.

ModuleNotFoundError: src... → use python -m src.<script>.

Retrieval weak? → expand dataset, re-run ingestion.

Browser TTS choppy? → choose a ro-RO voice; UI speaks sentence-by-sentence.

Costs → embeddings at ingestion; chat per request; optional server-side TTS/STT per request.

Roadmap
 Add unit tests for retrieval and tool routing

 Configurable models & top-K via .env

 Dockerfile and devcontainer

 Basic analytics (latency, hit rate)

 Optional auth for the web UI